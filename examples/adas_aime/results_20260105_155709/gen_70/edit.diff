--- a/original.py
+++ b/original.py
@@ -1,97 +1,105 @@
 """Agent design evaluation on math tasks."""
 
 import re
 from typing import Callable, List, Optional, Tuple, Dict
 from collections import Counter, defaultdict
 from math_eval import agent_evaluation
 
 
 # EVOLVE-BLOCK-START
 class Agent:
     def __init__(
         self,
         query_llm: Callable,
         temperature=0.0,
     ):
         self.output_format_instructions = (
             "On the final line output only the digits of the answer (0â€‘999). "
             "Provide your final answer enclosed in a LaTeX \\boxed{{...}} command."
         )
         self.query_llm = query_llm
         self.temperature = temperature
 
     def forward(self, problem: str) -> Tuple[str, float]:
         """Queries the LLM with a math problem and applies multi-step reflection."""
+        # Breaking down the problem into conceptual steps
         steps = self.break_down_problem(problem)
         responses = []
         total_cost = 0
 
         for step in steps:
             system_prompt, task_prompt = self.get_prompt_for_task(step)
             response, cost = self.query_llm(
                 prompt=task_prompt,
                 system=system_prompt,
                 temperature=self.temperature,
             )
             responses.append(response)
             total_cost += cost
 
         final_answer = self.refine_answer(responses)
-        return final_answer, total_cost
+        return final_answer.strip(), total_cost
 
     def break_down_problem(self, problem: str) -> List[str]:
         """Break down the problem into several steps for detailed reasoning."""
-        # Analyze the problem statement for keywords and structure the problem
+        # Analyzing the problem statement for keywords and structure
+        categories = {
+            "rectangle": "Identify the properties of rectangles.",
+            "dodecagon": "Analyze relationships in the dodecagon.",
+            "circle": "Review properties related to circles.",
+            "triangle": "Consider relationships among triangle elements."
+        }
+        
         steps = []
-        if "rectangle" in problem.lower():
-            steps.append("Identify the properties of the rectangles involved.")
-        if "circle" in problem.lower():
-            steps.append("Analyze the relationships between points on the circle.")
-        # Add more breakdown rules based on common problem types
-        steps.append(problem)  # Include the original problem as the last step
+        for keyword, instruction in categories.items():
+            if keyword in problem.lower():
+                steps.append(instruction)
+        
+        steps.append(problem)  # Include the original problem as the last step.
         return steps
 
     def get_prompt_for_task(self, problem: str) -> Tuple[str, str]:
         system_prompt = "You are a skilled mathematician."
         task_prompt = f"{self.output_format_instructions}:\n\nPlease solve the following problem step-by-step:\n\n{problem}\n\n"
         return system_prompt, task_prompt
 
     def refine_answer(self, responses: List[str]) -> str:
         """Iteratively refine the final answer based on multiple responses."""
-        current_answer = max(set(responses), key=responses.count)  # Start with the most common response
-        for _ in range(2):  # Allow two iterations of refinement
-            verification_prompt = (
-                f"Please review your answer to the following problem:\n\n"
-                f"Your answer is: {current_answer}\n\n"
-                f"Is this correct? If not, please provide corrections."
-            )
+        response_count = defaultdict(int)  # Count occurrences of each response
+
+        for response in responses:
+            response_count[response] += 1
+
+        # Identify the most frequent response
+        current_answer, _ = max(response_count.items(), key=lambda item: item[1])
+        for _ in range(2):  # Allow for two iterations of self-reflection
+            verification_prompt = f"Please review your answer to the following problem:\n\nYour answer is: {current_answer}\n\nIs this correct? If not, please provide corrections."
             verification_response, _ = self.query_llm(
                 prompt=verification_prompt,
                 system="You are a skilled mathematician.",
                 temperature=self.temperature,
             )
             if "incorrect" in verification_response.lower():
-                # Update current_answer based on feedback
-                current_answer = verification_response.strip()
+                current_answer = verification_response.strip()  # Update based on feedback
         return current_answer
 
 # EVOLVE-BLOCK-END
 
 
 def run_experiment(**kwargs):
     from utils import query_llm, create_call_limited_query_llm
     from functools import partial
 
     # Create base query_llm function
     base_query_llm = partial(query_llm, model_name=kwargs["model_name"])
 
     # Wrap it with call limiting (max 10 calls per forward pass)
     limited_query_llm = create_call_limited_query_llm(
         base_query_llm,
         max_calls=kwargs["max_calls"],
     )
 
     accuracy, cost_total, processed, num_llm_calls, df = agent_evaluation(
         Agent, limited_query_llm, year=kwargs["year"]
     )
     return accuracy, cost_total, processed, num_llm_calls, df