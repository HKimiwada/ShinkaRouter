<NAME>
adaptive_temperature_config
</NAME>

<DESCRIPTION>
Refactor the agent to use an adaptive temperature setting based on problem characteristics, ensuring that the LLM can balance creativity and accuracy more effectively on complex math problems while keeping the code structure intuitive.
</DESCRIPTION>

<DIFF>
<<<<<<< SEARCH
    def forward(self, problem: str) -> tuple[str, float]:
        """Queries the LLM with a math problem."""
        system_prompt, task_prompt = self.get_prompt_for_task(problem)
        responses = []
        costs = []
        for temp in [0.0, 0.5, 1.0]:
            response, cost = self.query_llm(
                prompt=task_prompt,
                system=system_prompt,
                temperature=self.temperature,
            )
        return response, cost
=======
    def forward(self, problem: str) -> tuple[str, float]:
        """Queries the LLM with a math problem."""
        system_prompt, task_prompt = self.get_prompt_for_task(problem)
        self.temperature = self.adaptive_temperature(problem)  # Set temperature based on problem type
        response, cost = self.query_llm(
            prompt=task_prompt,
            system=system_prompt,
            temperature=self.temperature,
        )
        return response.strip(), cost

    def adaptive_temperature(self, problem: str) -> float:
        """Adjust temperature based on the complexity of the problem."""
        complexity_keywords = ["complex", "difficult", "challenging", "geometry", "trigonometry"]
        if any(keyword in problem.lower() for keyword in complexity_keywords):
            return 0.7  # Higher creativity for complex problems
        return 0.3  # Lower creativity for simpler problems
>>>>>>> REPLACE
</DIFF>

<NAME>
iterative_verification_process
</NAME>

<DESCRIPTION>
Add an iterative verification mechanism that prompts the agent to reevaluate and improve its reasoning after each answer generation. This refinement process will help ensure mathematical accuracy and coherence in responses.
</DESCRIPTION>

<DIFF>
<<<<<<< SEARCH
        return response, cost
=======
        response = self.iterative_verification(response, system_prompt, task_prompt)
        return response, cost

    def iterative_verification(self, response: str, system_prompt: str, task_prompt: str) -> str:
        """Refines the response through iterative verification and reflection."""
        for attempt in range(2):  # Allow two attempts for refinement
            verification_prompt = (
                f"Review the following response to the problem:\n\n{task_prompt}\n"
                f"Your answer is: {response}\n\n"
                f"Is there a mistake or an improvement needed? Please revise your explanation and answer."
            )
            verification_response, _ = self.query_llm(
                prompt=verification_prompt,
                system=system_prompt,
                temperature=self.temperature,
            )
            if self.is_valid_response(verification_response):
                response = verification_response  # Update response if verified
            else:
                break  # Exit if no more revisions are needed
        return response
>>>>>>> REPLACE
</DIFF>

<NAME>
enhanced_task_prompt
</NAME>

<DESCRIPTION>
Revise the task prompt to emphasize thorough reasoning and logical checks after each step, directing the model to examine its conclusions to ensure alignment with the problem's requirements.
</DESCRIPTION>

<DIFF>
<<<<<<< SEARCH
        task_prompt = f"{self.output_format_instructions}:\n\nPlease solve the following problem step-by-step, showing all your reasoning. After each step, summarize your findings and check if they align with the problem requirements:\n\n{problem}\n\n"
=======
        task_prompt = (
            f"{self.output_format_instructions}:\n\n"
            f"Please solve the following problem step-by-step. After each step, review your logic and confirm it aligns with the problem requirements:\n\n{problem}\n\n"
        )
>>>>>>> REPLACE
</DIFF>