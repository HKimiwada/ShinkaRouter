"""Agent design evaluation on math tasks."""

import re
from typing import Callable, Tuple
from math_eval import agent_evaluation


# EVOLVE-BLOCK-START
class Agent:
    def __init__(self, query_llm: Callable, temperature=0.5):
        self.output_format_instructions = (
            "On the final line output only the digits of the answer (0â€‘999). "
            "Provide your final answer enclosed in a LaTeX \\boxed{{...}} command."
        )
        self.query_llm = query_llm
        self.temperature = temperature
        self.max_attempts = 3  # Limit retries for efficiency

    def forward(self, problem: str) -> Tuple[str, float]:
        """Queries the LLM with a math problem."""
        system_prompt, task_prompt = self.get_prompt_for_task(problem)

        for attempt in range(self.max_attempts):
            response, cost = self.query_llm(
                prompt=task_prompt,
                system=system_prompt,
                temperature=self.adjust_temperature(attempt),
            )
            if self.is_valid_response(response):
                return response.strip(), cost

            # Refine the prompt based on the last response feedback
            task_prompt = self.refine_prompt(response)

        return "", 0.0  # Return empty if all attempts fail

    def get_prompt_for_task(self, problem: str) -> Tuple[str, str]:
        system_prompt = "You are a skilled mathematician."
        task_prompt = (
            f"{self.output_format_instructions}:\n"
            f"Please solve the following problem step-by-step, verifying your logic:\n\n{problem}\n\n"
        )
        return system_prompt, task_prompt

    def adjust_temperature(self, attempt: int) -> float:
        """Adjust temperature based on the number of attempts made, enhancing creativity gradually."""
        return min(1.0, 0.5 + (0.1 * attempt))  # Increase temperature with attempts

    def is_valid_response(self, response: str) -> bool:
        """Check if the response meets the formatting or correctness criteria."""
        return bool(re.match(r'^\d{1,3}$', response.strip()))

    def refine_prompt(self, last_response: str) -> str:
        """Create a refined prompt based on the last response's logic to enhance the next attempt."""
        return (
            f"In your last attempt, your answer was '{last_response}'. "
            "Please examine your reasoning and provide a revised answer if necessary."
        )

# EVOLVE-BLOCK-END


def run_experiment(**kwargs):
    from utils import query_llm, create_call_limited_query_llm
    from functools import partial

    # Create base query_llm function for the specified model
    base_query_llm = partial(query_llm, model_name=kwargs["model_name"])

    # Wrap with call limiting for max 10 calls per forward pass
    limited_query_llm = create_call_limited_query_llm(
        base_query_llm,
        max_calls=kwargs["max_calls"],
    )

    accuracy, cost_total, processed, num_llm_calls, df = agent_evaluation(
        Agent, limited_query_llm, year=kwargs["year"]
    )
    return accuracy, cost_total, processed, num_llm_calls, df