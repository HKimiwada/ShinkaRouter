--- a/original.py
+++ b/original.py
@@ -1,74 +1,81 @@
 """Agent design evaluation on math tasks."""
 
 import re
 from typing import Callable, List, Optional, Tuple, Dict
 from collections import Counter, defaultdict
 from math_eval import agent_evaluation
 
 
 # EVOLVE-BLOCK-START
 class Agent:
-    def __init__(
-        self,
-        query_llm: Callable,
-        temperature=0.0,
-    ):
+    def __init__(self, query_llm: Callable, temperature=0.0):
         self.output_format_instructions = (
             "On the final line output only the digits of the answer (0-999). "
-            "Provide your final answer enclosed in a LaTeX \\boxed{{...}} command. "
-            "Additionally, summarize the steps taken to reach this conclusion."
+            "Provide your final answer enclosed in a LaTeX \\boxed{{...}} command."
         )
         self.query_llm = query_llm
         self.temperature = temperature
 
-    def forward(self, problem: str) -> tuple[str, float]:
-        """Queries the LLM with a math problem."""
+    def forward(self, problem: str) -> Tuple[str, float]:
+        """Queries the LLM with a math problem and performs enhanced multi-step reasoning."""
         system_prompt, task_prompt = self.get_prompt_for_task(problem)
-        response, cost = None, None
+        responses = []
+        costs = []
+
+        # Adjust temperature based on problem complexity
         complexity_keywords = ["complex", "difficult", "challenging"]
         complexity_score = sum(keyword in problem.lower() for keyword in complexity_keywords)
-        self.temperature = min(1.0, 0.5 + 0.2 * complexity_score)  # Adjust temperature based on complexity
+        self.temperature = min(1.0, 0.5 + 0.2 * complexity_score)
+
         for attempt in range(3):  # Limit to 3 attempts for reflection
-            for temp in [0.0, 0.5, 1.0]:
-                responses = []
-        costs = []
-        for temp in [0.0, 0.5, 1.0]:  # Generate responses with different temperatures
             response, cost = self.query_llm(
-            prompt=task_prompt,
-            system=system_prompt,
-            temperature=self.temperature,
-        )
+                prompt=task_prompt,
+                system=system_prompt,
+                temperature=self.temperature,
+            )
             responses.append(response)
             costs.append(cost)
 
-        # Select the best response based on a simple scoring mechanism
+            # Prepare the reflection prompt for improved reasoning
+            reflection_prompt = (
+                f"Review the following answer:\n{response}\n"
+                f"Are there any mistakes? If so, correct the mistakes."
+            )
+            reflection_response, _ = self.query_llm(
+                prompt=reflection_prompt,
+                system=system_prompt,
+                temperature=self.temperature,
+            )
+            # Update the task prompt for the next iteration
+            task_prompt = f"{self.output_format_instructions}:\n\n{problem}\n\nReflection: {reflection_response}\n"
+
+        # Use the most common response as the final answer
         final_answer = max(set(responses), key=responses.count)
         total_cost = sum(costs)
         return final_answer, total_cost
 
-    def get_prompt_for_task(self, problem: str) -> tuple[str, str]:
+    def get_prompt_for_task(self, problem: str) -> Tuple[str, str]:
         system_prompt = "You are a skilled mathematician."
-        task_prompt = f"{self.output_format_instructions}:\n\nPlease solve the following problem step-by-step, showing all your reasoning. After each step, summarize your findings and check if they align with the problem requirements:\n\n{problem}\n\n"
+        task_prompt = f"{self.output_format_instructions}:\n\n{problem}\n\n"
         return system_prompt, task_prompt
-
 
 # EVOLVE-BLOCK-END
 
 
 def run_experiment(**kwargs):
     from utils import query_llm, create_call_limited_query_llm
     from functools import partial
 
     # Create base query_llm function
     base_query_llm = partial(query_llm, model_name=kwargs["model_name"])
 
     # Wrap it with call limiting (max 10 calls per forward pass)
     limited_query_llm = create_call_limited_query_llm(
         base_query_llm,
         max_calls=kwargs["max_calls"],
     )
 
     accuracy, cost_total, processed, num_llm_calls, df = agent_evaluation(
         Agent, limited_query_llm, year=kwargs["year"]
     )
     return accuracy, cost_total, processed, num_llm_calls, df