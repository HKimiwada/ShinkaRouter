<NAME>
multi_step_reflection
</NAME>

<DESCRIPTION>
Implement a dynamic reflection mechanism that enables the agent to review and refine its responses across multiple steps. This improvement encourages iterative reasoning and allows the agent to catch potential mistakes early. By adding a mechanism where the agent assesses its preliminary answer and checks it against problem requirements after each decision, it can enhance the quality and accuracy of its final submission.
</DESCRIPTION>

<DIFF>
<<<<<<< SEARCH
        response, cost = self.query_llm(
=======
        initial_response, cost = self.query_llm(
        )
        for i in range(3):  # Reflect and revise the response iteratively
            reflection_prompt = f"Evaluate your previous answer: {initial_response}. Ensure it meets the problem requirements and assumptions and refine it as necessary."
            response, cost = self.query_llm(
>>>>>>> REPLACE
            prompt=task_prompt,
            system=system_prompt,
            temperature=self.temperature,
        )
        return response, cost

    def get_prompt_for_task(self, problem: str) -> tuple[str, str]:
        system_prompt = "You are a skilled mathematician."
        task_prompt = f"{self.output_format_instructions}:\n\nPlease solve the following problem step-by-step, showing all your reasoning. After each step, summarize your findings and check if they align with the problem requirements:\n\n{problem}\n\n"
        return system_prompt, task_prompt
=======
        return final_answer, cost
>>>>>>> REPLACE

</DIFF>

<NAME>
exemplary_responses
</NAME>

<DESCRIPTION>
Integrate a mechanism for using exemplary responses or few-shot examples specific to certain problem types. These examples provide the model with clearer context and goals, allowing the LLM to generate more relevant answers aligned with the problem's requirements. This change aims to encourage better understanding and solution construction based on past successes.
</DESCRIPTION>

<DIFF>
<<<<<<< SEARCH
    def get_prompt_for_task(self, problem: str) -> tuple[str, str]:
=======
    def get_prompt_for_task(self, problem: str) -> tuple[str, str, List[str]]:
        exemplary_responses = [
            "When faced with similar AIME configurations, I approached the problem using combinatorial analysis or geometric reasoning.",
            "Another successful strategy involved breaking down the problem into smaller sub-problems, solving them sequentially."
        ]
>>>>>>> REPLACE
        system_prompt = "You are a skilled mathematician."
        task_prompt = f"{self.output_format_instructions}:\n\nPlease solve the following problem step-by-step, showing all your reasoning. After each step, summarize your findings and check if they align with the problem requirements:\n\n{problem}\n\n"
        return system_prompt, task_prompt, exemplary_responses
>>>>>>> REPLACE


    def forward(self, problem: str) -> tuple[str, float]:
        """Queries the LLM with a math problem."""
        system_prompt, task_prompt, examples = self.get_prompt_for_task(problem)
=======
        task_prompt += f"\nHere are some past successful response strategies relevant to similar problems:\n{examples}\n"
>>>>>>> REPLACE

        response, cost = self.query_llm(
            prompt=task_prompt,
            system=system_prompt,
            temperature=self.temperature,
        )
        return response, cost

</DIFF>