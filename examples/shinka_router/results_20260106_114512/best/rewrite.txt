"""ShinkaRouter: Agent with improved routing primitives for AIME problems, combining best practices from crossover and previous versions."""

import re
from typing import Callable, Tuple, List, Optional
from collections import Counter

class Agent:
    """Agent with adaptive routing primitives for AIME problems."""
    
    def __init__(
        self,
        query_llm: Callable,
        quick_temp: float = 0.7,
        deep_temp: float = 0.0,
        verify_temp: float = 0.0,
        ensemble_size: int = 3,
    ):
        self.query_llm = query_llm
        self.quick_temp = quick_temp
        self.deep_temp = deep_temp
        self.verify_temp = verify_temp
        self.ensemble_size = ensemble_size
        self._primitive_calls: List[str] = []
        self.output_format = (
            "On the final line output only the digits of the answer (0-999). "
            "Provide your final answer enclosed in a LaTeX \\boxed{{...}} command."
        )

    def _track_call(self, name: str) -> None:
        self._primitive_calls.append(name)

    def get_primitive_calls(self) -> List[str]:
        return self._primitive_calls.copy()

    def reset_tracking(self) -> None:
        self._primitive_calls = []

    @staticmethod
    def extract_boxed_answer(response: str) -> Optional[str]:
        idx = response.rfind("\\boxed")
        if idx < 0:
            idx = response.rfind("\\fbox")
        if idx < 0:
            return None
        brace_idx = response.find("{", idx)
        if brace_idx < 0:
            return None
        level = 0
        for i in range(brace_idx, len(response)):
            if response[i] == "{":
                level += 1
            elif response[i] == "}":
                level -= 1
                if level == 0:
                    content = response[brace_idx + 1:i]
                    content = content.strip().lstrip("0") or "0"
                    return content
        return None

    # Primitive methods
    def baseline_solve(self, problem: str) -> Tuple[str, float]:
        self._track_call("baseline_solve")
        prompt = f"{self.output_format}:\n\n{problem}\n\n"
        return self.query_llm(prompt, system="You are a skilled mathematician.", temperature=0.0)

    def quick_solve(self, problem: str) -> Tuple[str, float]:
        self._track_call("quick_solve")
        prompt = f"{self.output_format}\n\n{problem}\n\n"
        return self.query_llm(prompt, system="You are a skilled mathematician.", temperature=self.quick_temp)

    def deep_think(self, problem: str) -> Tuple[str, float]:
        self._track_call("deep_think")
        prompt = (
            f"{self.output_format}\n\n"
            f"Solve this problem carefully with detailed reasoning:\n\n{problem}"
        )
        return self.query_llm(prompt, system="You are an expert mathematician. Think step-by-step, showing all reasoning.", temperature=self.deep_temp)

    def verify(self, problem: str, response: str) -> Tuple[str, float]:
        self._track_call("verify")
        prompt = (
            f"{self.output_format}\n\n"
            f"Problem: {problem}\n\n"
            f"Solution:\n{response}\n\n"
            "Verify this solution step-by-step. If errors are found, correct and provide the final answer."
        )
        return self.query_llm(prompt, system="You are a rigorous mathematics professor.", temperature=self.verify_temp)

    def python_calc(self, problem: str) -> Tuple[str, float]:
        self._track_call("python_calc")
        prompt = (
            f"{self.output_format}\n\n"
            f"Solve this problem with detailed calculations:\n\n{problem}\n\n"
            "Show all intermediate steps clearly."
        )
        return self.query_llm(prompt, system="You are a systematic mathematician.", temperature=self.deep_temp)

    def ensemble_vote(self, problem: str, n: Optional[int] = None) -> Tuple[str, float]:
        self._track_call("ensemble_vote")
        n = n or self.ensemble_size
        prompt = f"{self.output_format}\n\n{problem}\n\n"
        responses = []
        answers = []
        total_cost = 0.0
        ensemble_temp = 0.5
        for _ in range(n):
            resp, cost = self.query_llm(prompt, system="You are a skilled mathematician.", temperature=ensemble_temp)
            responses.append(resp)
            total_cost += cost
            ans = self.extract_boxed_answer(resp)
            if ans:
                answers.append(ans)
        if answers:
            count = Counter(answers)
            winner, _ = count.most_common(1)[0]
            for resp in responses:
                if self.extract_boxed_answer(resp) == winner:
                    return resp, total_cost
        return responses[0], total_cost

    def self_critique(self, problem: str, draft: str) -> Tuple[str, float]:
        self._track_call("self_critique")
        prompt = (
            f"{self.output_format}\n\n"
            f"Problem: {problem}\n\n"
            f"Draft Solution:\n{draft}\n\n"
            "Review and improve the solution if needed."
        )
        return self.query_llm(prompt, system="You are a mathematician reviewing your work.", temperature=self.deep_temp)

    def estimate_difficulty(self, problem: str) -> Tuple[str, float]:
        self._track_call("estimate_difficulty")
        prompt = (
            f"Analyze this problem and classify difficulty as easy, medium, or hard:\n\n{problem}"
        )
        response, cost = self.query_llm(prompt, system="You are an expert at evaluating problem difficulty.", temperature=0.0)
        diff = response.strip().lower()
        if "easy" in diff:
            return "easy", cost
        elif "hard" in diff:
            return "hard", cost
        else:
            return "medium", cost

    def classify_problem_type(self, problem: str) -> Tuple[str, float]:
        self._track_call("classify_problem_type")
        prompt = (
            f"Classify this problem into one category: algebra, geometry, number_theory, combinatorics, calculus."
        )
        response, cost = self.query_llm(prompt, system="You are an expert problem classifier.", temperature=0.0)
        ptype = response.strip().lower().replace(" ", "_")
        for category in ["algebra", "geometry", "number_theory", "combinatorics", "calculus"]:
            if category in ptype:
                return category, cost
        return "algebra", cost

    # Main routing logic with adaptive strategy
    def forward(self, problem: str) -> Tuple[str, float]:
        self.reset_tracking()
        total_cost = 0.0
        # Step 1: Estimate difficulty
        difficulty, cost_diff = self.estimate_difficulty(problem)
        total_cost += cost_diff
        # Step 2: Route based on difficulty
        if difficulty == "easy":
            resp, c = self.baseline_solve(problem)
            total_cost += c
        elif difficulty == "medium":
            resp, c = self.deep_think(problem)
            total_cost += c
            ver_resp, c2 = self.verify(problem, resp)
            total_cost += c2
            resp = ver_resp
        else:  # hard
            resp, c = self.deep_think(problem)
            total_cost += c
            crit_resp, c2 = self.self_critique(problem, resp)
            total_cost += c2
            ver_resp, c3 = self.verify(problem, crit_resp)
            total_cost += c3
            resp = ver_resp
        return resp, total_cost

def run_experiment(**kwargs):
    from utils import query_llm, create_call_limited_query_llm
    from functools import partial
    from config import ROUTER_CONFIG
    base_llm = partial(query_llm, model_name=kwargs["model_name"])
    limited_llm = create_call_limited_query_llm(base_llm, max_calls=kwargs["max_calls"])
    from math_eval import agent_evaluation
    return agent_evaluation(
        Agent, limited_llm, year=kwargs["year"],
        quick_temp=ROUTER_CONFIG.quick_temp,
        deep_temp=ROUTER_CONFIG.deep_temp,
        verify_temp=ROUTER_CONFIG.verify_temp,
        ensemble_size=ROUTER_CONFIG.ensemble_size,
    )