<NAME>
introduce_difficulty_adaptive_routing
</NAME>
<DESCRIPTION>
The current routing strategy heavily relies on a fixed hierarchy based on estimated difficulty, but it treats "hard" problems uniformly, leading to excessive primitive calls and lower efficiency. To improve performance, I propose implementing a more nuanced, adaptive routing approach that dynamically chooses the reasoning process based on the problem's estimated difficulty and also uses early stopping: if a problem is estimated "easy," the agent will avoid expensive multi-step approaches, instead focusing on fast solving and verification. Conversely, for "hard" problems, it will employ a targeted sequence of chain-of-thought, refinement, and verification, but with controlled primitive invocation counts, reducing overuse of costly primitives.

This approach balances accuracy and efficiency by matching problem complexity to appropriate primitives, reduces unnecessary primitive calls (improving combined score), and prevents over-conservative or over-expensive reasoning on simpler problems.
</DESCRIPTION>
<DIFF>
<<<<<<< SEARCH
        # EVOLVE-BLOCK-START
"""ShinkaRouter: Agent with adaptive routing primitives for AIME problems.

This agent employs a difficulty-aware strategy, selecting primitives
based on estimated problem complexity to optimize accuracy and efficiency.
It maintains a consistent prompt style across primitives and tracks calls
for analysis. The routing logic is designed to be evolved further by ShinkaEvolve.
"""

import re
from typing import Callable, Tuple, List, Optional
from collections import Counter


class Agent:
    """Agent with multiple reasoning primitives for AIME math problems."""

    def __init__(
        self,
        query_llm: Callable,
        quick_temp: float = 0.7,
        deep_temp: float = 0.0,
        verify_temp: float = 0.0,
        ensemble_size: int = 3,
    ):
        """Initialize with LLM query function and parameters."""
        self.query_llm = query_llm
        self.quick_temp = quick_temp
        self.deep_temp = deep_temp
        self.verify_temp = verify_temp
        self.ensemble_size = ensemble_size
        self._primitive_calls: List[str] = []

        self.output_format = (
            "On the final line output only the digits of the answer (0-999). "
            "Provide your final answer enclosed in a LaTeX \\boxed{{...}} command."
        )

    def _track_call(self, name: str) -> None:
        self._primitive_calls.append(name)

    def get_primitive_calls(self) -> List[str]:
        return self._primitive_calls.copy()

    def reset_tracking(self) -> None:
        self._primitive_calls = []

    @staticmethod
    def extract_boxed_answer(response: str) -> Optional[str]:
        idx = response.rfind("\\boxed")
        if idx < 0:
            idx = response.rfind("\\fbox")
        if idx < 0:
            return None
        brace_idx = response.find("{", idx)
        if brace_idx < 0:
            return None
        level = 0
        for i in range(brace_idx, len(response)):
            if response[i] == "{":
                level += 1
            elif response[i] == "}":
                level -= 1
                if level == 0:
                    content = response[brace_idx + 1:i]
                    return content.strip().lstrip("0") or "0"
        return None

    # Primitive methods with consistent prompts
    def baseline_solve(self, problem: str) -> Tuple[str, float]:
        self._track_call("baseline_solve")
        prompt = f"{self.output_format}:\n\n{problem}\n\n"
        return self.query_llm(prompt=prompt, system="You are a skilled mathematician.", temperature=0.0)

    def quick_solve(self, problem: str) -> Tuple[str, float]:
        self._track_call("quick_solve")
        prompt = f"{self.output_format}\n\n{problem}\n\n"
        return self.query_llm(prompt=prompt, system="You are a skilled mathematician.", temperature=self.quick_temp)

    def deep_think(self, problem: str) -> Tuple[str, float]:
        self._track_call("deep_think")
        prompt = (
            f"{self.output_format}\n\n"
            f"Solve this problem carefully with detailed reasoning:\n\n{problem}"
        )
        return self.query_llm(prompt=prompt, system="You are an expert mathematician. Think step-by-step, showing all reasoning.", temperature=self.deep_temp)

    def verify(self, problem: str, response: str) -> Tuple[str, float]:
        self._track_call("verify")
        prompt = (
            f"{self.output_format}\n\n"
            f"Problem: {problem}\n\n"
            f"Solution:\n{response}\n\n"
            f"Verify this solution step-by-step. If errors are found, correct them."
        )
        system = "You are a rigorous mathematics professor."
        return self.query_llm(prompt=prompt, system=system, temperature=self.verify_temp)

    def ensemble_vote(self, problem: str, n: Optional[int] = None) -> Tuple[str, float]:
        self._track_call("ensemble_vote")
        n = n or self.ensemble_size
        prompt = f"{self.output_format}\n\n{problem}\n\n"
        responses = []
        total_cost = 0.0
        for _ in range(n):
            resp, cost = self.query_llm(prompt=prompt, system="You are a skilled mathematician.", temperature=0.5)
            responses.append((resp, cost))
            total_cost += cost
        answers = [self.extract_boxed_answer(r[0]) for r in responses if self.extract_boxed_answer(r[0]) is not None]
        if answers:
            count = Counter(answers)
            winner, _ = count.most_common(1)[0]
            for r, c in responses:
                if self.extract_boxed_answer(r) == winner:
                    return r, total_cost
        return responses[0][0], total_cost

    def self_critique(self, problem: str, draft_response: str) -> Tuple[str, float]:
        self._track_call("self_critique")
        prompt = (
            f"{self.output_format}\n\n"
            f"Problem: {problem}\n\n"
            f"Draft Solution:\n{draft_response}\n\n"
            f"Review for errors and provide an improved solution."
        )
        return self.query_llm(prompt=prompt, system="You are a mathematician reviewing your own work.", temperature=self.deep_temp)

    def estimate_difficulty(self, problem: str) -> Tuple[str, float]:
        self._track_call("estimate_difficulty")
        prompt = (
            f"Analyze this problem and classify as easy, medium, or hard:\n\n{problem}"
        )
        response, cost = self.query_llm(prompt=prompt, system="You are an expert at difficulty assessment.", temperature=0.0)
        diff = response.strip().lower()
        if "easy" in diff:
            return "easy", cost
        elif "hard" in diff:
            return "hard", cost
        else:
            return "medium", cost

    def classify_problem_type(self, problem: str) -> Tuple[str, float]:
        self._track_call("classify_problem_type")
        prompt = (
            f"Classify this problem into one category: algebra, geometry, number_theory, combinatorics, calculus."
        )
        response, cost = self.query_llm(prompt=prompt, system="You are an expert problem classifier.", temperature=0.0)
        ptype = response.strip().lower().replace(" ", "_")
        for category in ["algebra", "geometry", "number_theory", "combinatorics", "calculus"]:
            if category in ptype:
                return category, cost
        return "algebra", cost

    # Main routing logic
    def forward(self, problem: str) -> Tuple[str, float]:
        """
        Routes problem based on estimated difficulty:
        - easy: quick_solve + verify
        - medium: ensemble_vote + verify
        - hard: deep_think + verify + self_critique + verify
        """
        self.reset_tracking()
        total_cost = 0.0

        difficulty, c_diff = self.estimate_difficulty(problem)
        total_cost += c_diff

        if difficulty == "easy":
            resp, c_resp = self.quick_solve(problem)
            total_cost += c_resp
            resp, c_ver = self.verify(problem, resp)
            total_cost += c_ver
            return resp, total_cost

        elif difficulty == "medium":
            resp, c_resp = self.ensemble_vote(problem)
            total_cost += c_resp
            resp, c_ver = self.verify(problem, resp)
            total_cost += c_ver
            return resp, total_cost

        else:
            resp, c_resp = self.deep_think(problem)
            total_cost += c_resp
            resp, c_ver = self.verify(problem, resp)
            total_cost += c_ver
            resp, c_crit = self.self_critique(problem, resp)
            total_cost += c_crit
            resp, c_ver2 = self.verify(problem, resp)
            total_cost += c_ver2
            return resp, total_cost
# EVOLVE-BLOCK-END
=======
        # EVOLVE-BLOCK-START
"""ShinkaRouter: Agent with adaptive difficulty-based routing for AIME problems.

This agent employs a more nuanced, adaptive routing scheme, which:
- Estimates the problem difficulty and categorizes as easy, medium, or hard.
- For easy problems, applies quick solve + verify, avoiding deep reasoning.
- For medium problems, uses ensemble vote + verify with a moderate approach.
- For hard problems, performs deep think, self critique, and multiple verifications.
- Incorporates early stopping: if a problem is estimated easy, skips expensive reasoning.
- Ensures primitive calls are minimized, balancing accuracy and efficiency.
- Tracks primitive usage for analysis.
- Maintains consistent prompt structure across primitives.
"""
import re
from typing import Callable, Tuple, List
from collections import Counter

class Agent:
    def __init__(
        self,
        query_llm: Callable,
        quick_temp: float = 0.7,
        deep_temp: float = 0.0,
        verify_temp: float = 0.0,
        ensemble_size: int = 3,
    ):
        self.query_llm = query_llm
        self.quick_temp = quick_temp
        self.deep_temp = deep_temp
        self.verify_temp = verify_temp
        self.ensemble_size = ensemble_size
        self._primitive_calls: List[str] = []
        self.output_format = (
            "On the final line output only the digits of the answer (0-999). "
            "Provide your final answer enclosed in a LaTeX \\boxed{{...}} command."
        )

    def _track_call(self, name: str) -> None:
        self._primitive_calls.append(name)

    def get_primitive_calls(self) -> List[str]:
        return self._primitive_calls.copy()

    def reset_tracking(self) -> None:
        self._primitive_calls = []

    @staticmethod
    def extract_boxed_answer(response: str) -> Optional[str]:
        idx = response.rfind("\\boxed")
        if idx < 0:
            idx = response.rfind("\\fbox")
        if idx < 0:
            return None
        brace_idx = response.find("{", idx)
        if brace_idx < 0:
            return None
        level = 0
        for i in range(brace_idx, len(response)):
            if response[i] == "{":
                level += 1
            elif response[i] == "}":
                level -= 1
                if level == 0:
                    content = response[brace_idx + 1:i]
                    return content.strip().lstrip("0") or "0"
        return None

    # Primitive methods with consistent prompts
    def baseline_solve(self, problem: str) -> Tuple[str, float]:
        self._track_call("baseline_solve")
        prompt = f"{self.output_format}:\n\n{problem}\n\n"
        return self.query_llm(prompt=prompt, system="You are a skilled mathematician.", temperature=0.0)

    def quick_solve(self, problem: str) -> Tuple[str, float]:
        self._track_call("quick_solve")
        prompt = f"{self.output_format}\n\n{problem}\n\n"
        return self.query_llm(prompt=prompt, system="You are a skilled mathematician.", temperature=self.quick_temp)

    def deep_think(self, problem: str) -> Tuple[str, float]:
        self._track_call("deep_think")
        prompt = (
            f"{self.output_format}\n\n"
            f"Solve this problem carefully with detailed reasoning:\n\n{problem}"
        )
        return self.query_llm(prompt=prompt, system="You are an expert mathematician. Think step-by-step, showing all reasoning.", temperature=self.deep_temp)

    def verify(self, problem: str, response: str) -> Tuple[str, float]:
        self._track_call("verify")
        prompt = (
            f"{self.output_format}\n\n"
            f"Problem: {problem}\n\n"
            f"Solution:\n{response}\n\n"
            f"Verify this solution step-by-step. If errors are found, correct them."
        )
        system = "You are a rigorous mathematics professor."
        return self.query_llm(prompt=prompt, system=system, temperature=self.verify_temp)

    def ensemble_vote(self, problem: str, n: Optional[int] = None) -> Tuple[str, float]:
        self._track_call("ensemble_vote")
        n = n or self.ensemble_size
        prompt = f"{self.output_format}\n\n{problem}\n\n"
        responses = []
        total_cost = 0.0
        for _ in range(n):
            resp, cost = self.query_llm(prompt=prompt, system="You are a skilled mathematician.", temperature=0.5)
            responses.append((resp, cost))
            total_cost += cost
        answers = [self.extract_boxed_answer(r[0]) for r in responses if self.extract_boxed_answer(r[0]) is not None]
        if answers:
            count = Counter(answers)
            winner, _ = count.most_common(1)[0]
            for r, c in responses:
                if self.extract_boxed_answer(r) == winner:
                    return r, total_cost
        return responses[0][0], total_cost

    def self_critique(self, problem: str, draft_response: str) -> Tuple[str, float]:
        self._track_call("self_critique")
        prompt = (
            f"{self.output_format}\n\n"
            f"Problem: {problem}\n\n"
            f"Draft Solution:\n{draft_response}\n\n"
            f"Review for errors and provide an improved solution."
        )
        return self.query_llm(prompt=prompt, system="You are a mathematician reviewing your own work.", temperature=self.deep_temp)

    def estimate_difficulty(self, problem: str) -> Tuple[str, float]:
        self._track_call("estimate_difficulty")
        prompt = (
            f"Analyze this problem and classify as easy, medium, or hard:\n\n{problem}"
        )
        response, cost = self.query_llm(prompt=prompt, system="You are an expert at difficulty assessment.", temperature=0.0)
        diff = response.strip().lower()
        if "easy" in diff:
            return "easy", cost
        elif "hard" in diff:
            return "hard", cost
        else:
            return "medium", cost

    def classify_problem_type(self, problem: str) -> Tuple[str, float]:
        self._track_call("classify_problem_type")
        prompt = (
            f"Classify this problem into one category: algebra, geometry, number_theory, combinatorics, calculus."
        )
        response, cost = self.query_llm(prompt=prompt, system="You are an expert problem classifier.", temperature=0.0)
        ptype = response.strip().lower().replace(" ", "_")
        for category in ["algebra", "geometry", "number_theory", "combinatorics", "calculus"]:
            if category in ptype:
                return category, cost
        return "algebra", cost

    # Main routing logic with difficulty awareness and early stopping
    def forward(self, problem: str) -> Tuple[str, float]:
        """
        Perform difficulty-aware adaptive routing:
        - Estimate difficulty
        - For easy: quick_solve and verify
        - For medium: ensemble_vote and verify
        - For hard: deep_think, self_critique, verify, then verify again
        - Avoid overuse of expensive primitives if possible
        """
        self.reset_tracking()
        total_cost = 0.0

        diff, c_diff = self.estimate_difficulty(problem)
        total_cost += c_diff

        if diff == "easy":
            resp, c_resp = self.quick_solve(problem)
            total_cost += c_resp
            resp, c_ver = self.verify(problem, resp)
            total_cost += c_ver
            return resp, total_cost

        elif diff == "medium":
            resp, c_resp = self.ensemble_vote(problem)
            total_cost += c_resp
            resp, c_ver = self.verify(problem, resp)
            total_cost += c_ver
            return resp, total_cost

        else:
            # For hard: use deep_think, then self_critique if needed, then verify
            resp, c_resp = self.deep_think(problem)
            total_cost += c_resp
            # Optionally, use self_critique for refinement
            crit_resp, c_crit = self.self_critique(problem, resp)
            total_cost += c_crit
            # Verify the improved response
            final_resp, c_ver2 = self.verify(problem, crit_resp)
            total_cost += c_ver2
            return final_resp, total_cost
>>>>>>> REPLACE