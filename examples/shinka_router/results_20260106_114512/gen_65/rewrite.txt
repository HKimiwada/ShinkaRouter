"""ShinkaRouter: Enhanced adaptive routing agent combining best primitives for AIME problems.

This agent utilizes a layered, difficulty-based approach, integrating multiple reasoning primitives
with consistent prompts. It dynamically routes problems to maximize accuracy and efficiency, using
self-critique, ensemble voting, and verification strategies. The design is inspired by the crossover
of previous implementations, aiming for improved performance and robustness.
"""

import re
from typing import Callable, Tuple, List, Optional
from collections import Counter

class Agent:
    def __init__(
        self,
        query_llm: Callable,
        quick_temp: float = 0.7,
        deep_temp: float = 0.0,
        verify_temp: float = 0.0,
        ensemble_size: int = 3,
    ):
        self.query_llm = query_llm
        self.quick_temp = quick_temp
        self.deep_temp = deep_temp
        self.verify_temp = verify_temp
        self.ensemble_size = ensemble_size
        self._primitive_calls: List[str] = []

        self.output_format = (
            "On the final line output only the digits of the answer (0-999). "
            "Provide your final answer enclosed in a LaTeX \\boxed{{...}} command."
        )

    def _track_call(self, name: str) -> None:
        self._primitive_calls.append(name)

    def get_primitive_calls(self) -> List[str]:
        return self._primitive_calls.copy()

    def reset_tracking(self) -> None:
        self._primitive_calls.clear()

    @staticmethod
    def extract_boxed_answer(response: str) -> Optional[str]:
        idx = response.rfind("\\boxed")
        if idx < 0:
            idx = response.rfind("\\fbox")
        if idx < 0:
            return None
        brace_idx = response.find("{", idx)
        if brace_idx < 0:
            return None
        level = 0
        for i in range(brace_idx, len(response)):
            if response[i] == "{":
                level += 1
            elif response[i] == "}":
                level -= 1
                if level == 0:
                    content = response[brace_idx + 1:i]
                    return content.strip().lstrip("0") or "0"
        return None

    # Primitive methods with uniform prompts
    def baseline_solve(self, problem: str) -> Tuple[str, float]:
        self._track_call("baseline_solve")
        prompt = f"{self.output_format}\n\n{problem}\n\n"
        return self.query_llm(prompt=prompt, system="You are a skilled mathematician.", temperature=0.0)

    def quick_solve(self, problem: str) -> Tuple[str, float]:
        self._track_call("quick_solve")
        prompt = f"{self.output_format}\n\n{problem}\n\n"
        return self.query_llm(prompt=prompt, system="You are a skilled mathematician.", temperature=self.quick_temp)

    def deep_think(self, problem: str) -> Tuple[str, float]:
        self._track_call("deep_think")
        prompt = (
            f"{self.output_format}\n\n"
            f"Solve this problem carefully with detailed reasoning:\n\n{problem}"
        )
        return self.query_llm(prompt=prompt, system="You are an expert mathematician. Think step-by-step, showing all reasoning.", temperature=self.deep_temp)

    def verify(self, problem: str, response: str) -> Tuple[str, float]:
        self._track_call("verify")
        prompt = (
            f"{self.output_format}\n\n"
            f"Problem: {problem}\n\n"
            f"Solution:\n{response}\n\n"
            "Verify this solution thoroughly. If errors are found, correct and finalize."
        )
        return self.query_llm(prompt=prompt, system="You are a rigorous mathematics professor.", temperature=self.verify_temp)

    def python_calc(self, problem: str) -> Tuple[str, float]:
        self._track_call("python_calc")
        prompt = (
            f"{self.output_format}\n\n"
            f"Break down the calculations step-by-step:\n\n{problem}\n\n"
            f"Show all intermediate steps clearly."
        )
        return self.query_llm(prompt=prompt, system="You are a systematic mathematician.", temperature=self.deep_temp)

    def ensemble_vote(self, problem: str, n: Optional[int] = None) -> Tuple[str, float]:
        self._track_call("ensemble_vote")
        n = n or self.ensemble_size
        prompt = f"{self.output_format}\n\n{problem}\n\n"
        responses = []
        answers = []
        total_cost = 0.0
        ensemble_temp = 0.5
        for _ in range(n):
            resp, c = self.query_llm(prompt=prompt, system="You are a skilled mathematician.", temperature=ensemble_temp)
            responses.append(resp)
            total_cost += c
            ans = self.extract_boxed_answer(resp)
            if ans:
                answers.append(ans)
        if answers:
            count = Counter(answers)
            winner, _ = count.most_common(1)[0]
            for r in responses:
                if self.extract_boxed_answer(r) == winner:
                    return r, total_cost
        return responses[0], total_cost

    def self_critique(self, problem: str, draft: str) -> Tuple[str, float]:
        self._track_call("self_critique")
        prompt = (
            f"{self.output_format}\n\n"
            f"Problem: {problem}\n\n"
            f"Draft Solution:\n{draft}\n\n"
            "Review and improve this solution."
        )
        return self.query_llm(prompt=prompt, system="You are a mathematician reviewing your work.", temperature=self.deep_temp)

    def estimate_difficulty(self, problem: str) -> Tuple[str, float]:
        self._track_call("estimate_difficulty")
        prompt = (
            f"Analyze this problem and classify its difficulty as easy, medium, or hard:\n\n{problem}"
        )
        response, cost = self.query_llm(prompt=prompt, system="You are an AIME difficulty assessor.", temperature=0.0)
        diff = response.strip().lower()
        if "easy" in diff:
            return "easy", cost
        elif "hard" in diff:
            return "hard", cost
        else:
            return "medium", cost

    def classify_problem_type(self, problem: str) -> Tuple[str, float]:
        self._track_call("classify_problem_type")
        prompt = (
            "Classify this problem into one category: algebra, geometry, number_theory, combinatorics, calculus."
        )
        response, cost = self.query_llm(prompt=prompt, system="You are an expert problem classifier.", temperature=0.0)
        ptype = response.strip().lower().replace(" ", "_")
        for cat in ["algebra", "geometry", "number_theory", "combinatorics", "calculus"]:
            if cat in ptype:
                return cat, cost
        return "algebra", cost

    def forward(self, problem: str) -> Tuple[str, float]:
        """Layered, difficulty-based routing with self-critique and ensemble."""
        self.reset_tracking()
        total_cost = 0.0

        # Step 1: Estimate difficulty
        difficulty, c_diff = self.estimate_difficulty(problem)
        total_cost += c_diff

        # Step 2: Route based on difficulty
        if difficulty == "easy":
            resp, c = self.baseline_solve(problem)
            total_cost += c
        elif difficulty == "medium":
            resp, c = self.deep_think(problem)
            total_cost += c
            vresp, vc = self.verify(problem, resp)
            total_cost += vc
            return vresp, total_cost
        else:
            # For 'hard' problems, perform multiple deep_think + self_critique + verify
            draft, c1 = self.deep_think(problem)
            total_cost += c1
            for _ in range(2):
                crit, c2 = self.self_critique(problem, draft)
                total_cost += c2
                vresp, vc = self.verify(problem, crit)
                total_cost += vc
                # if verification is confident, break early
                if "correct" in vresp.lower() or "yes" in vresp.lower():
                    return vresp, total_cost
                draft = crit
            return vresp, total_cost
        return resp, total_cost
# END