"""ShinkaRouter: Adaptive difficulty-based routing for AIME problems.

This agent classifies problems into 'easy', 'medium', or 'hard' using a dedicated
difficulty estimator. It then applies:
- 'easy': baseline_solve (1 call)
- 'medium': deep_think + verify (2 calls)
- 'hard': ensemble_vote + verify (4 calls)
This approach minimizes primitive usage on easier problems and maximizes accuracy for difficult ones.
"""

import re
from typing import Callable, Tuple, List, Optional
from collections import Counter

class Agent:
    def __init__(
        self,
        query_llm: Callable,
        quick_temp: float = 0.7,
        deep_temp: float = 0.0,
        verify_temp: float = 0.0,
        ensemble_size: int = 3,
    ):
        self.query_llm = query_llm
        self.quick_temp = quick_temp
        self.deep_temp = deep_temp
        self.verify_temp = verify_temp
        self.ensemble_size = ensemble_size
        self.output_format = (
            "On the final line output only the digits of the answer (0-999). "
            "Provide your final answer enclosed in a LaTeX \\boxed{{...}} command."
        )
        self._primitive_calls: List[str] = []

    def _track_call(self, name: str) -> None:
        self._primitive_calls.append(name)

    def get_primitive_calls(self) -> List[str]:
        return self._primitive_calls.copy()

    def reset_tracking(self) -> None:
        self._primitive_calls = []

    @staticmethod
    def extract_boxed_answer(response: str) -> Optional[str]:
        idx = response.rfind("\\boxed")
        if idx < 0:
            idx = response.rfind("\\fbox")
        if idx < 0:
            return None
        brace_idx = response.find("{", idx)
        if brace_idx < 0:
            return None
        level = 0
        for i in range(brace_idx, len(response)):
            if response[i] == "{":
                level += 1
            elif response[i] == "}":
                level -= 1
                if level == 0:
                    content = response[brace_idx + 1:i]
                    content = content.strip().lstrip("0") or "0"
                    return content
        return None

    def baseline_solve(self, problem: str) -> Tuple[str, float]:
        self._track_call("baseline_solve")
        prompt = f"{self.output_format}:\n\n{problem}\n\n"
        return self.query_llm(prompt, system="You are a skilled mathematician.", temperature=0.0)

    def deep_think(self, problem: str) -> Tuple[str, float]:
        self._track_call("deep_think")
        prompt = (
            f"{self.output_format}\n\n"
            f"Solve this problem carefully with detailed reasoning:\n\n{problem}"
        )
        return self.query_llm(prompt, system="You are an expert mathematician. Think step-by-step, show all reasoning.", temperature=self.deep_temp)

    def verify(self, problem: str, response: str) -> Tuple[str, float]:
        self._track_call("verify")
        prompt = (
            f"{self.output_format}\n\n"
            f"Problem: {problem}\n\n"
            f"Solution:\n{response}\n\n"
            "Verify this solution carefully. If errors are found, correct them and provide the final answer."
        )
        return self.query_llm(prompt, system="You are a rigorous mathematics professor.", temperature=self.verify_temp)

    def ensemble_vote(self, problem: str, n: Optional[int] = None) -> Tuple[str, float]:
        self._track_call("ensemble_vote")
        n = n or self.ensemble_size
        prompt = f"{self.output_format}\n\n{problem}\n\n"
        responses = []
        total_cost = 0.0
        for _ in range(n):
            resp, cost = self.query_llm(prompt, system="You are a skilled mathematician.", temperature=0.5)
            responses.append(resp)
            total_cost += cost
        answers = [self.extract_boxed_answer(r) for r in responses if self.extract_boxed_answer(r)]
        if answers:
            count = Counter(answers)
            winner, _ = count.most_common(1)[0]
            for r in responses:
                if self.extract_boxed_answer(r) == winner:
                    return r, total_cost
        return responses[0], total_cost

    def estimate_difficulty(self, problem: str) -> Tuple[str, float]:
        self._track_call("estimate_difficulty")
        prompt = (
            f"Analyze this AIME problem and classify its difficulty as easy, medium, or hard:\n\n{problem}"
        )
        response, cost = self.query_llm(prompt, system="You are an expert at evaluating AIME problem difficulty.", temperature=0.0)
        diff = response.strip().lower()
        if "easy" in diff:
            return "easy", cost
        elif "hard" in diff:
            return "hard", cost
        else:
            return "medium", cost

    def forward(self, problem: str) -> Tuple[str, float]:
        self.reset_tracking()
        total_cost = 0.0
        difficulty, diff_cost = self.estimate_difficulty(problem)
        total_cost += diff_cost
        if difficulty == "easy":
            resp, c = self.baseline_solve(problem)
            total_cost += c
            return resp, total_cost
        elif difficulty == "medium":
            resp, c = self.deep_think(problem)
            total_cost += c
            ver_resp, ver_c = self.verify(problem, resp)
            total_cost += ver_c
            return ver_resp, total_cost
        else:
            resp, c = self.ensemble_vote(problem, n=4)
            total_cost += c
            ver_resp, ver_c = self.verify(problem, resp)
            total_cost += ver_c
            return ver_resp, total_cost