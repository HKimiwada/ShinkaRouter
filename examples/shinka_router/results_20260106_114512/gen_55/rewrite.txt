"""ShinkaRouter: Improved adaptive, difficulty-based routing agent for AIME problems.

Utilizes estimate_difficulty to guide primitive selection, balancing accuracy and efficiency.
Applies verification and self-critique in complex cases, and uses simple solve when straightforward.
Leverages consistent prompts and tracks primitive calls for analysis and tuning.
"""

import re
from typing import Callable, Tuple, List, Optional

class Agent:
    def __init__(
        self,
        query_llm: Callable,
        quick_temp: float = 0.7,
        deep_temp: float = 0.0,
        verify_temp: float = 0.0,
        ensemble_size: int = 3,
    ):
        self.query_llm = query_llm
        self.quick_temp = quick_temp
        self.deep_temp = deep_temp
        self.verify_temp = verify_temp
        self.ensemble_size = ensemble_size
        self._primitive_calls: List[str] = []

        self.output_format = (
            "On the final line output only the digits of the answer (0-999). "
            "Provide your final answer enclosed in a LaTeX \\boxed{{...}} command."
        )

    def _track_call(self, name: str) -> None:
        self._primitive_calls.append(name)

    def get_primitive_calls(self) -> List[str]:
        return self._primitive_calls.copy()

    def reset_tracking(self) -> None:
        self._primitive_calls.clear()

    @staticmethod
    def extract_boxed_answer(response: str) -> Optional[str]:
        idx = response.rfind("\\boxed")
        if idx < 0:
            idx = response.rfind("\\fbox")
        if idx < 0:
            return None
        brace_idx = response.find("{", idx)
        if brace_idx < 0:
            return None
        level = 0
        for i in range(brace_idx, len(response)):
            if response[i] == "{":
                level += 1
            elif response[i] == "}":
                level -= 1
                if level == 0:
                    content = response[brace_idx + 1:i]
                    return content.strip().lstrip("0") or "0"
        return None

    # Primitive methods with reliable, consistent prompts
    def baseline_solve(self, problem: str) -> Tuple[str, float]:
        self._track_call("baseline_solve")
        prompt = f"{self.output_format}\n\n{problem}\n\n"
        return self.query_llm(prompt, system="You are a skilled mathematician.", temperature=0.0)

    def deep_think(self, problem: str) -> Tuple[str, float]:
        self._track_call("deep_think")
        prompt = (
            f"{self.output_format}\n\n"
            f"Solve this problem carefully with detailed reasoning:\n\n{problem}"
        )
        return self.query_llm(prompt, system="You are an expert mathematician. Think step-by-step, show all reasoning.", temperature=self.deep_temp)

    def verify(self, problem: str, response: str) -> Tuple[str, float]:
        self._track_call("verify")
        prompt = (
            f"{self.output_format}\n\n"
            f"Problem: {problem}\n\n"
            f"Solution:\n{response}\n\n"
            "Verify this solution carefully and provide the correct answer if errors are found."
        )
        return self.query_llm(prompt, system="You are a rigorous professor.", temperature=self.verify_temp)

    def ensemble_vote(self, problem: str, n: Optional[int]=None) -> Tuple[str, float]:
        self._track_call("ensemble_vote")
        n = n or self.ensemble_size
        prompt = f"{self.output_format}\n\n{problem}\n\n"
        responses = []
        total_cost = 0.0
        for _ in range(n):
            resp, c = self.query_llm(prompt, system="You are a skilled mathematician.", temperature=0.5)
            responses.append((resp, c))
            total_cost += c
        answers = [self.extract_boxed_answer(r[0]) for r in responses if self.extract_boxed_answer(r[0])]
        if answers:
            from collections import Counter
            count = Counter(answers)
            winner, _ = count.most_common(1)[0]
            for r, c in responses:
                if self.extract_boxed_answer(r) == winner:
                    return r, total_cost
        return responses[0][0], total_cost

    def self_critique(self, problem: str, draft: str) -> Tuple[str, float]:
        self._track_call("self_critique")
        prompt = (
            f"{self.output_format}\n\n"
            f"Problem: {problem}\n\n"
            f"Draft Solution:\n{draft}\n\n"
            "Review and improve this solution."
        )
        return self.query_llm(prompt, system="You are a mathematician reviewing your own work.", temperature=self.deep_temp)

    def estimate_difficulty(self, problem: str) -> Tuple[str, float]:
        self._track_call("estimate_difficulty")
        prompt = (
            f"Analyze this problem and classify as easy, medium, or hard:\n\n{problem}"
        )
        response, c = self.query_llm(prompt, system="Difficulty classifier.", temperature=0.0)
        diff = response.strip().lower()
        if "easy" in diff:
            return "easy", c
        elif "hard" in diff:
            return "hard", c
        else:
            return "medium", c

    def classify_problem_type(self, problem: str) -> Tuple[str, float]:
        self._track_call("classify_problem_type")
        prompt = (
            f"Classify this problem into one category: algebra, geometry, number_theory, combinatorics, calculus."
        )
        response, c = self.query_llm(prompt, system="Problem classifier.", temperature=0.0)
        ptype = response.strip().lower()
        for cat in ["algebra", "geometry", "number_theory", "combinatorics", "calculus"]:
            if cat in ptype:
                return cat, c
        return "algebra", c

    def forward(self, problem: str) -> Tuple[str, float]:
        """Layered routing guided by estimated difficulty."""
        self.reset_tracking()
        total_cost = 0.0

        # Step 1: estimate the difficulty
        difficulty, c_diff = self.estimate_difficulty(problem)
        total_cost += c_diff

        # Step 2: route based on difficulty
        if difficulty == "easy":
            resp, c = self.baseline_solve(problem)
            total_cost += c
        elif difficulty == "medium":
            # Use deep_think + verification
            resp, c = self.deep_think(problem)
            total_cost += c
            resp2, c2 = self.verify(problem, resp)
            total_cost += c2
            # Optional: self-critique if needed (skipped for simplicity)
            resp = resp2
        else:  # hard
            # Use ensemble, verify, and self-critique
            resp, c = self.ensemble_vote(problem)
            total_cost += c
            resp2, c2 = self.verify(problem, resp)
            total_cost += c2
            resp3, c3 = self.self_critique(problem, resp2)
            total_cost += c3
            resp_final, c4 = self.verify(problem, resp3)
            total_cost += c4
            resp = resp_final

        return resp, total_cost
# END OF IMPROVEMENT