"""ShinkaRouter: Agent with improved routing primitives for AIME problems.

This agent combines detailed primitive implementations with adaptive routing
based on problem difficulty, ensemble voting, and verification to improve
accuracy and efficiency.

FIXES from previous:
- Uses consistent prompt structure across primitives
- Implements difficulty-based routing with fallback
- Adds ensemble voting for medium problems
- Incorporates verification after solutions
- Tracks primitive calls for analysis
"""

import re
from typing import Callable, Tuple, List, Optional
from collections import Counter

class Agent:
    """Agent with multiple reasoning primitives for AIME problems."""

    def __init__(
        self,
        query_llm: Callable,
        quick_temp: float = 0.7,
        deep_temp: float = 0.0,
        verify_temp: float = 0.0,
        ensemble_size: int = 3,
    ):
        self.query_llm = query_llm
        self.quick_temp = quick_temp
        self.deep_temp = deep_temp
        self.verify_temp = verify_temp
        self.ensemble_size = ensemble_size
        self._primitive_calls: List[str] = []

        self.output_format = (
            "On the final line output only the digits of the answer (0-999). "
            "Provide your final answer enclosed in a LaTeX \\boxed{{...}} command."
        )

    def _track_call(self, primitive_name: str) -> None:
        self._primitive_calls.append(primitive_name)

    def get_primitive_calls(self) -> List[str]:
        return self._primitive_calls.copy()

    def reset_tracking(self) -> None:
        self._primitive_calls = []

    @staticmethod
    def extract_boxed_answer(response: str) -> Optional[str]:
        idx = response.rfind("\\boxed")
        if idx < 0:
            idx = response.rfind("\\fbox")
        if idx < 0:
            return None
        brace_idx = response.find("{", idx)
        if brace_idx < 0:
            return None
        level = 0
        for i in range(brace_idx, len(response)):
            if response[i] == "{":
                level += 1
            elif response[i] == "}":
                level -= 1
                if level == 0:
                    content = response[brace_idx + 1:i]
                    content = content.strip().lstrip("0") or "0"
                    return content
        return None

    # Primitive methods with consistent prompts
    def baseline_solve(self, problem: str) -> Tuple[str, float]:
        self._track_call("baseline_solve")
        prompt = f"{self.output_format}:\n\n{problem}\n\n"
        response, cost = self.query_llm(prompt=prompt, system="You are a skilled mathematician.", temperature=0.0)
        return response, cost

    def quick_solve(self, problem: str) -> Tuple[str, float]:
        self._track_call("quick_solve")
        prompt = f"{self.output_format}\n\n{problem}\n\n"
        response, cost = self.query_llm(prompt=prompt, system="You are a skilled mathematician.", temperature=self.quick_temp)
        return response, cost

    def deep_think(self, problem: str) -> Tuple[str, float]:
        self._track_call("deep_think")
        prompt = (
            f"{self.output_format}\n\n"
            f"Solve this problem carefully with detailed reasoning:\n\n{problem}"
        )
        response, cost = self.query_llm(prompt=prompt, system="You are an expert mathematician. Think step-by-step, showing all reasoning.", temperature=self.deep_temp)
        return response, cost

    def verify(self, problem: str, candidate_response: str) -> Tuple[str, float]:
        self._track_call("verify")
        prompt = (
            f"{self.output_format}\n\n"
            f"Problem: {problem}\n\n"
            f"Student's Solution:\n{candidate_response}\n\n"
            f"Verify this solution step-by-step. If errors are found, correct them and provide the final answer."
        )
        response, cost = self.query_llm(prompt=prompt, system="You are a rigorous mathematics professor.", temperature=self.verify_temp)
        return response, cost

    def python_calc(self, problem: str) -> Tuple[str, float]:
        self._track_call("python_calc")
        prompt = (
            f"{self.output_format}\n\n"
            f"Break down the calculations step-by-step:\n\n{problem}\n\n"
            f"Show all intermediate steps clearly."
        )
        response, cost = self.query_llm(prompt=prompt, system="You are a systematic mathematician.", temperature=self.deep_temp)
        return response, cost

    def ensemble_vote(self, problem: str, n: Optional[int] = None) -> Tuple[str, float]:
        self._track_call("ensemble_vote")
        n = n or self.ensemble_size
        prompt = f"{self.output_format}\n\n{problem}\n\n"
        responses = []
        answers = []
        total_cost = 0.0
        ensemble_temp = 0.5
        for _ in range(n):
            resp, cost = self.query_llm(prompt=prompt, system="You are a skilled mathematician.", temperature=ensemble_temp)
            responses.append(resp)
            total_cost += cost
            ans = self.extract_boxed_answer(resp)
            if ans:
                answers.append(ans)
        if answers:
            vote_counts = Counter(answers)
            winner, _ = vote_counts.most_common(1)[0]
            for resp in responses:
                if self.extract_boxed_answer(resp) == winner:
                    return resp, total_cost
        return responses[0], total_cost

    def self_critique(self, problem: str, draft_response: str) -> Tuple[str, float]:
        self._track_call("self_critique")
        prompt = (
            f"{self.output_format}\n\n"
            f"Problem: {problem}\n\n"
            f"Draft Solution:\n{draft_response}\n\n"
            f"Review for errors and provide an improved answer."
        )
        response, cost = self.query_llm(prompt=prompt, system="You are a mathematician reviewing your work.", temperature=self.deep_temp)
        return response, cost

    def estimate_difficulty(self, problem: str) -> Tuple[str, float]:
        self._track_call("estimate_difficulty")
        prompt = (
            f"Analyze this problem and classify difficulty as easy, medium, or hard:\n\n{problem}"
        )
        response, cost = self.query_llm(prompt=prompt, system="You are an expert at evaluating problem difficulty.", temperature=0.0)
        diff = response.strip().lower()
        if "easy" in diff:
            return "easy", cost
        elif "hard" in diff:
            return "hard", cost
        else:
            return "medium", cost

    def classify_problem_type(self, problem: str) -> Tuple[str, float]:
        self._track_call("classify_problem_type")
        prompt = (
            f"Classify this problem into one category: algebra, geometry, number_theory, combinatorics, calculus:\n\n{problem}"
        )
        response, cost = self.query_llm(prompt=prompt, system="You are an expert problem classifier.", temperature=0.0)
        ptype = response.strip().lower().replace(" ", "_")
        for category in ["algebra", "geometry", "number_theory", "combinatorics", "calculus"]:
            if category in ptype:
                return category, cost
        return "algebra", cost

    # Main routing logic with adaptive difficulty-based approach
    def forward(self, problem: str) -> Tuple[str, float]:
        self.reset_tracking()
        total_cost = 0.0
        # Step 1: Classify difficulty
        difficulty, diff_cost = self.estimate_difficulty(problem)
        total_cost += diff_cost
        # Step 2: Route based on difficulty
        if difficulty == "easy":
            response, cost = self.baseline_solve(problem)
        elif difficulty == "medium":
            response, cost = self.deep_think(problem)
            v_response, v_cost = self.verify(problem, response)
            response = v_response
            cost += v_cost
        else:
            # For hard, try ensemble voting then fallback
            response, e_cost = self.ensemble_vote(problem)
            total_cost += e_cost
            v_response, v_cost = self.verify(problem, response)
            response = v_response
            total_cost += v_cost
        return response, total_cost