"""ShinkaRouter: Improved adaptive routing for AIME problems.

This version:
- Uses a simple, difficulty-based routing with minimal calls.
- For easy problems: baseline + verify.
- For medium: deep_think + verify.
- For hard: ensemble_vote + verify.
- Avoids unnecessary multiple self_critique or repeated deep_think calls.
- Strives for high accuracy with low primitive usage.
"""

import re
from typing import Tuple, List

class Agent:
    def __init__(
        self,
        query_llm: Callable,
        quick_temp: float = 0.7,
        deep_temp: float = 0.0,
        verify_temp: float = 0.0,
        ensemble_size: int = 3,
    ):
        self.query_llm = query_llm
        self.quick_temp = quick_temp
        self.deep_temp = deep_temp
        self.verify_temp = verify_temp
        self.ensemble_size = ensemble_size
        self._primitive_calls: List[str] = []

        self.output_format = (
            "On the final line output only the digits of the answer (0-999). "
            "Provide your final answer enclosed in a LaTeX \\boxed{{...}} command."
        )

    def _track_call(self, name: str):
        self._primitive_calls.append(name)

    def reset_tracking(self):
        self._primitive_calls.clear()

    def extract_boxed_answer(self, response: str) -> Optional[str]:
        idx = response.rfind("\\boxed")
        if idx < 0:
            idx = response.rfind("\\fbox")
        if idx < 0:
            return None
        brace_idx = response.find("{", idx)
        if brace_idx < 0:
            return None
        level = 0
        for i in range(brace_idx, len(response)):
            if response[i] == "{":
                level += 1
            elif response[i] == "}":
                level -= 1
                if level == 0:
                    content = response[brace_idx + 1:i]
                    content = content.strip().lstrip("0") or "0"
                    return content
        return None

    def baseline_solve(self, problem: str) -> Tuple[str, float]:
        self._track_call("baseline_solve")
        prompt = f"{self.output_format}:\n\n{problem}\n\n"
        response, cost = self.query_llm(prompt=prompt, system="You are a skilled mathematician.", temperature=0.0)
        return response, cost

    def deep_think(self, problem: str) -> Tuple[str, float]:
        self._track_call("deep_think")
        prompt = (
            f"{self.output_format}\n\n"
            f"Solve this problem carefully with detailed reasoning:\n\n{problem}"
        )
        response, cost = self.query_llm(prompt=prompt, system="You are an expert mathematician.", temperature=0.0)
        return response, cost

    def verify(self, problem: str, response: str) -> Tuple[str, float]:
        self._track_call("verify")
        prompt = (
            f"{self.output_format}\n\n"
            f"Problem: {problem}\n\n"
            f"Student's Solution:\n{response}\n\n"
            f"Verify this solution. Check each step for errors and provide the correct final answer."
        )
        response_verified, cost = self.query_llm(prompt=prompt, system="You are a rigorous professor.", temperature=0.0)
        return response_verified, cost

    def ensemble_vote(self, problem: str, n: int = None) -> Tuple[str, float]:
        self._track_call("ensemble_vote")
        n = n or self.ensemble_size
        prompt = f"{self.output_format}\n\n{problem}\n\n"
        responses = []
        total_cost = 0.0
        for _ in range(n):
            resp, c = self.query_llm(prompt=prompt, system="You are a skilled mathematician.", temperature=0.5)
            responses.append(resp)
            total_cost += c
        answers = [self.extract_boxed_answer(r) for r in responses if self.extract_boxed_answer(r)]
        if answers:
            from collections import Counter
            count = Counter(answers)
            winner, _ = count.most_common(1)[0]
            for r in responses:
                if self.extract_boxed_answer(r) == winner:
                    return r, total_cost
        return responses[0], total_cost

    def forward(self, problem: str) -> Tuple[str, float]:
        self.reset_tracking()
        total_cost = 0.0

        # Step 1: estimate difficulty
        self._track_call("estimate_difficulty")
        diff_resp, c_diff = self.query_llm(
            prompt=(
                "Analyze this AIME problem and classify its difficulty.\n\n"
                f"Problem: {problem}\n\n"
                "Respond with: easy, medium, or hard."
            ),
            system="You are an expert at evaluating difficulty.",
            temperature=0.0
        )
        total_cost += c_diff
        diff = diff_resp.strip().lower()
        if "easy" in diff:
            # Easy: baseline + verify
            resp, c1 = self.baseline_solve(problem)
            total_cost += c1
            v_resp, c2 = self.verify(problem, resp)
            total_cost += c2
            return v_resp, total_cost
        elif "medium" in diff:
            # Medium: deep_think + verify
            resp, c1 = self.deep_think(problem)
            total_cost += c1
            v_resp, c2 = self.verify(problem, resp)
            total_cost += c2
            return v_resp, total_cost
        else:
            # Hard: ensemble + verify
            resp, c1 = self.ensemble_vote(problem, n=3)
            total_cost += c1
            v_resp, c2 = self.verify(problem, resp)
            total_cost += c2
            return v_resp, total_cost
</CODE>