# The new rewritten program here.
"""ShinkaRouter: Agent with simplified, efficiency-minded routing for AIME."""

import re
from typing import Tuple

class Agent:
    def __init__(self, query_llm):
        self.query_llm = query_llm
        self._primitive_calls = []

    def reset_tracking(self):
        self._primitive_calls = []

    def _track_call(self, name: str):
        self._primitive_calls.append(name)

    def extract_boxed_answer(self, response: str) -> str:
        idx = response.rfind("\\boxed")
        if idx < 0:
            idx = response.rfind("\\fbox")
        if idx < 0:
            return ""
        brace_idx = response.find("{", idx)
        if brace_idx < 0:
            return ""
        level = 0
        for i in range(brace_idx, len(response)):
            if response[i] == "{":
                level += 1
            elif response[i] == "}":
                level -= 1
                if level == 0:
                    content = response[brace_idx + 1:i]
                    content = content.strip().lstrip("0") or "0"
                    return content
        return ""

    def baseline_solve(self, problem: str) -> Tuple[str, float]:
        self._track_call("baseline_solve")
        prompt = (
            "You are a skilled mathematician.\n"
            "Output only the digits (0-999) enclosed in \\boxed{}.\n\n"
            f"{problem}"
        )
        response, cost = self.query_llm(prompt=prompt, temperature=0.0)
        return response, cost

    def deep_think(self, problem: str) -> Tuple[str, float]:
        self._track_call("deep_think")
        prompt = (
            "You are an expert mathematician. Think step-by-step, "
            "showing all your reasoning before arriving at the final answer.\n\n"
            f"{problem}"
        )
        response, cost = self.query_llm(prompt=prompt, temperature=0.0)
        return response, cost

    def verify(self, problem: str, full_response: str) -> Tuple[str, float]:
        self._track_call("verify")
        prompt = (
            "You are a rigorous mathematics professor checking a student's work.\n"
            "Carefully verify the solution step by step.\n"
            "If there's an error, explain and provide the correct answer.\n\n"
            f"Problem: {problem}\n"
            f"Solution:\n{full_response}\n"
            "Verify and give the final answer enclosed in \\boxed{}."
        )
        response, cost = self.query_llm(prompt=prompt, temperature=0.0)
        return response, cost

    def forward(self, problem: str) -> Tuple[str, float]:
        self.reset_tracking()
        total_cost = 0.0

        # Step 1: Use baseline for initial answer
        response, cost = self.baseline_solve(problem)
        total_cost += cost

        # Extract answer and verify correctness
        answer = self.extract_boxed_answer(response)
        verify_response, v_cost = self.verify(problem, response)
        total_cost += v_cost

        # Decide whether to trust or re-solve with deeper reasoning
        # For simplicity, if answer looks invalid, reattempt via deep_think
        if answer == "" or len(answer) > 3:
            # For harder problems, do a more detailed step-by-step
            deep_response, dc = self.deep_think(problem)
            total_cost += dc
            # Verify again
            verify_response, v_cost2 = self.verify(problem, deep_response)
            total_cost += v_cost2
            # Return the improved solution
            return verify_response, total_cost
        else:
            # Accept initial answer if verified
            return response, total_cost