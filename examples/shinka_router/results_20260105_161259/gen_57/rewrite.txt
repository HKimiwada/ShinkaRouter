"""ShinkaRouter: Agent with routing primitives for AIME problems.

This agent provides a toolbox of specialized primitives that can be
composed in different ways. The forward() method contains the routing
logic that will be evolved by ShinkaEvolve.
"""

import re
from typing import Callable, Tuple, List, Optional, Dict
from collections import Counter


class Agent:
    """Agent with multiple reasoning primitives for AIME math problems."""

    def __init__(
        self,
        query_llm: Callable,
        quick_temp: float = 0.7,
        deep_temp: float = 0.0,
        verify_temp: float = 0.0,
        ensemble_size: int = 3,
    ):
        """
        Initialize the routing agent.

        Args:
            query_llm: Function to query the LLM
            quick_temp: Temperature for quick_solve
            deep_temp: Temperature for deep_think
            verify_temp: Temperature for verify
            ensemble_size: Number of samples for ensemble voting
        """
        self.query_llm = query_llm
        self.quick_temp = quick_temp
        self.deep_temp = deep_temp
        self.verify_temp = verify_temp
        self.ensemble_size = ensemble_size

        # Standard output format for AIME
        self.output_format = (
            "On the final line output only the digits of the answer (0-999). "
            "Provide your final answer enclosed in a LaTeX \\boxed{{...}} command."
        )

        # Primitive call tracking (reset per forward() call)
        self._primitive_calls: List[str] = []

    def _track_call(self, primitive_name: str) -> None:
        """Track a primitive call for analysis."""
        self._primitive_calls.append(primitive_name)

    def get_primitive_calls(self) -> List[str]:
        """Return list of primitives called in this forward pass."""
        return self._primitive_calls.copy()

    def reset_tracking(self) -> None:
        """Reset primitive call tracking."""
        self._primitive_calls = []

    @staticmethod
    def extract_boxed_answer(response: str) -> Optional[str]:
        """Extract answer from \\boxed{} in response."""
        idx = response.rfind("\\boxed")
        if idx < 0:
            idx = response.rfind("\\fbox")
        if idx < 0:
            return None

        brace_idx = response.find("{", idx)
        if brace_idx < 0:
            return None

        level = 0
        for i in range(brace_idx, len(response)):
            if response[i] == "{":
                level += 1
            elif response[i] == "}":
                level -= 1
                if level == 0:
                    content = response[brace_idx + 1:i]
                    # Clean and normalize
                    content = content.strip().lstrip("0") or "0"
                    return content
        return None

    # ========================================================================
    # PRIMITIVE METHODS (Stable - Not Evolved)
    # ========================================================================

    def baseline_solve(self, problem: str) -> Tuple[str, float]:
        """Exact replication of adas_aime baseline behavior."""
        self._track_call("baseline_solve")

        system_prompt = "You are a skilled mathematician."
        task_prompt = f"{self.output_format}:\n\n{problem}\n\n"

        response, cost = self.query_llm(
            prompt=task_prompt,
            system=system_prompt,
            temperature=0.0,
        )
        return response, cost

    def quick_solve(self, problem: str) -> Tuple[str, float]:
        """Fast solving with higher temperature for quick guessing."""
        self._track_call("quick_solve")

        system_prompt = "You are a skilled mathematician. Solve quickly and efficiently."
        task_prompt = f"{self.output_format}\n\n{problem}\n\n"

        response, cost = self.query_llm(
            prompt=task_prompt,
            system=system_prompt,
            temperature=self.quick_temp,
        )
        return response, cost

    def deep_think(self, problem: str) -> Tuple[str, float]:
        """Careful reasoning with step-by-step analysis."""
        self._track_call("deep_think")

        system_prompt = (
            "You are an expert mathematician. Think step-by-step, "
            "showing all your reasoning before arriving at the final answer."
        )
        task_prompt = (
            f"Solve this problem carefully with detailed reasoning:\n\n"
            f"{problem}\n\n"
            f"{self.output_format}"
        )

        response, cost = self.query_llm(
            prompt=task_prompt,
            system=system_prompt,
            temperature=self.deep_temp,
        )
        return response, cost

    def verify(self, problem: str, candidate_answer: str) -> Tuple[str, float]:
        """Verify a proposed solution."""
        self._track_call("verify")

        system_prompt = (
            "You are a rigorous mathematics professor checking a student's answer. "
            "Verify if the answer is correct. If wrong, provide the correct answer."
        )
        task_prompt = (
            f"Problem: {problem}\n\n"
            f"Proposed Answer: {candidate_answer}\n\n"
            f"Is this answer correct? If not, what is the correct answer?\n"
            f"{self.output_format}"
        )

        response, cost = self.query_llm(
            prompt=task_prompt,
            system=system_prompt,
            temperature=self.verify_temp,
        )
        return response, cost

    def ensemble_vote(self, problem: str, n: Optional[int] = None) -> Tuple[str, float]:
        """Generate solutions and take majority vote on the answer."""
        self._track_call("ensemble_vote")

        if n is None:
            n = self.ensemble_size

        system_prompt = "You are a skilled mathematician."
        task_prompt = f"{self.output_format}\n\n{problem}\n\n"

        responses = []
        answers = []
        total_cost = 0.0

        for _ in range(n):
            response, cost = self.query_llm(
                prompt=task_prompt,
                system=system_prompt,
                temperature=0.7,
            )
            responses.append(response)
            total_cost += cost

            ans = self.extract_boxed_answer(response)
            if ans:
                answers.append(ans)

        # Majority vote
        if answers:
            vote_counts = Counter(answers)
            winner, count = vote_counts.most_common(1)[0]

            for resp in responses:
                extracted = self.extract_boxed_answer(resp)
                if extracted == winner:
                    return resp, total_cost

        return responses[0], total_cost

    def estimate_difficulty(self, problem: str) -> Tuple[str, float]:
        """Estimate problem difficulty as 'easy', 'medium', or 'hard'."""
        self._track_call("estimate_difficulty")

        system_prompt = "You are an expert at evaluating AIME problem difficulty."
        task_prompt = (
            f"Analyze this AIME problem and classify its difficulty.\n\n"
            f"Problem: {problem}\n\n"
            f"Respond with exactly one word: easy, medium, or hard"
        )

        response, cost = self.query_llm(
            prompt=task_prompt,
            system=system_prompt,
            temperature=0.0,
        )

        difficulty = response.strip().lower()
        return (difficulty if difficulty in ['easy', 'medium', 'hard'] else 'medium'), cost

    # ========================================================================
    # ROUTING LOGIC (Evolved by ShinkaEvolve)
    # ========================================================================

    # EVOLVE-BLOCK-START
    def forward(self, problem: str) -> Tuple[str, float]:
        """Main entry point - routes problem to appropriate primitives."""
        self.reset_tracking()
        difficulty, diff_cost = self.estimate_difficulty(problem)

        if difficulty == 'easy':
            response, cost = self.quick_solve(problem)
            total_cost = cost + diff_cost
        elif difficulty == 'medium':
            response, cost = self.deep_think(problem)
            boxed_answer = self.extract_boxed_answer(response)
            if boxed_answer:
                verify_response, verify_cost = self.verify(problem, boxed_answer)
                total_cost = cost + verify_cost + diff_cost
                response = verify_response
            else:
                total_cost = cost + diff_cost
        else:  # hard
            response, cost = self.deep_think(problem)
            boxed_answer = self.extract_boxed_answer(response)
            if boxed_answer:
                verify_response, verify_cost = self.verify(problem, boxed_answer)
                total_cost = cost + verify_cost + diff_cost
                response = verify_response
            else:
                total_cost = cost + diff_cost

        return response, total_cost
    # EVOLVE-BLOCK-END


def run_experiment(**kwargs):
    """Entry point called by evaluate.py."""
    from utils import query_llm, create_call_limited_query_llm
    from functools import partial
    from config import ROUTER_CONFIG

    base_query_llm = partial(query_llm, model_name=kwargs["model_name"])
    limited_query_llm = create_call_limited_query_llm(
        base_query_llm,
        max_calls=kwargs["max_calls"],
    )

    from math_eval import agent_evaluation

    accuracy, cost_total, processed, num_llm_calls, df = agent_evaluation(
        Agent,
        limited_query_llm,
        year=kwargs["year"],
        quick_temp=ROUTER_CONFIG.quick_temp,
        deep_temp=ROUTER_CONFIG.deep_temp,
        verify_temp=ROUTER_CONFIG.verify_temp,
        ensemble_size=ROUTER_CONFIG.ensemble_size,
    )

    return accuracy, cost_total, processed, num_llm_calls, df