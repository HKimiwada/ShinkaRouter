"""ShinkaRouter: Agent with routing primitives for AIME problems.

This agent provides a toolbox of specialized primitives that can be
composed in different ways. The forward() method contains the routing
logic that will be evolved by ShinkaEvolve.
"""

import re
from typing import Callable, Tuple, List, Optional
from collections import Counter


class Agent:
    """Agent with multiple reasoning primitives for AIME math problems."""

    def __init__(
        self,
        query_llm: Callable,
        quick_temp: float = 0.7,
        deep_temp: float = 0.0,
        verify_temp: float = 0.0,
        ensemble_size: int = 3,
    ):
        self.query_llm = query_llm
        self.quick_temp = quick_temp
        self.deep_temp = deep_temp
        self.verify_temp = verify_temp
        self.ensemble_size = ensemble_size
        self.output_format = (
            "On the final line output only the digits of the answer (0-999). "
            "Provide your final answer enclosed in a LaTeX \\boxed{{...}} command."
        )
        self._primitive_calls: List[str] = []

    def _track_call(self, primitive_name: str) -> None:
        """Track a primitive call for analysis."""
        self._primitive_calls.append(primitive_name)

    def get_primitive_calls(self) -> List[str]:
        """Return list of primitives called in this forward pass."""
        return self._primitive_calls.copy()

    def reset_tracking(self) -> None:
        """Reset primitive call tracking."""
        self._primitive_calls = []

    @staticmethod
    def extract_boxed_answer(response: str) -> Optional[str]:
        """Extract answer from \\boxed{} in response."""
        idx = response.rfind("\\boxed")
        if idx < 0:
            idx = response.rfind("\\fbox")
        if idx < 0:
            return None

        brace_idx = response.find("{", idx)
        if brace_idx < 0:
            return None

        level = 0
        for i in range(brace_idx, len(response)):
            if response[i] == "{":
                level += 1
            elif response[i] == "}":
                level -= 1
                if level == 0:
                    content = response[brace_idx + 1:i]
                    content = content.strip().lstrip("0") or "0"
                    return content
        return None

    def baseline_solve(self, problem: str) -> Tuple[str, float]:
        """Use baseline method."""
        self._track_call("baseline_solve")

        system_prompt = "You are a skilled mathematician."
        task_prompt = f"{self.output_format}:\n\n{problem}\n\n"
        response, cost = self.query_llm(
            prompt=task_prompt,
            system=system_prompt,
            temperature=0.0,
        )
        return response, cost

    def deep_think(self, problem: str) -> Tuple[str, float]:
        """Careful reasoning with chain-of-thought and low temperature."""
        self._track_call("deep_think")

        system_prompt = (
            "You are an expert mathematician. Think step-by-step, "
            "showing all your reasoning before arriving at the final answer."
        )
        task_prompt = (
            f"Solve this problem carefully with detailed reasoning:\n\n"
            f"{problem}\n\n"
            f"{self.output_format}"
        )

        response, cost = self.query_llm(
            prompt=task_prompt,
            system=system_prompt,
            temperature=self.deep_temp,
        )
        return response, cost

    def verify(self, problem: str, candidate_answer: str) -> Tuple[str, float]:
        """Verify a proposed solution."""
        self._track_call("verify")

        system_prompt = (
            "You are a mathematics professor checking a student's answer. "
            "Verify if the answer is correct. If wrong, provide the correct answer."
        )
        task_prompt = (
            f"Problem: {problem}\n\n"
            f"Proposed Answer: {candidate_answer}\n\n"
            f"Is this answer correct? If not, what is the correct answer?\n"
            f"{self.output_format}"
        )

        response, cost = self.query_llm(
            prompt=task_prompt,
            system=system_prompt,
            temperature=self.verify_temp,
        )
        return response, cost

    def estimate_difficulty(self, problem: str) -> Tuple[str, float]:
        """Estimate problem difficulty."""
        self._track_call("estimate_difficulty")

        system_prompt = "You are an expert at evaluating AIME problem difficulty."
        task_prompt = (
            f"Analyze this AIME problem and classify its difficulty.\n\n"
            f"Problem: {problem}\n\n"
            f"Respond with one word: easy, medium, or hard."
        )

        response, cost = self.query_llm(
            prompt=task_prompt,
            system=system_prompt,
            temperature=0.0,
        )

        difficulty = response.strip().lower()
        return difficulty if difficulty in ['easy', 'medium', 'hard'] else 'medium', cost

    # EVOLVE-BLOCK-START
    def forward(self, problem: str) -> Tuple[str, float]:
        """Main entry point - routes problem to appropriate primitives."""
        self.reset_tracking()
        difficulty, diff_cost = self.estimate_difficulty(problem)

        if difficulty == 'easy':
            response, cost = self.baseline_solve(problem)
        elif difficulty == 'medium':
            response, deep_cost = self.deep_think(problem)
            boxed_answer = self.extract_boxed_answer(response)
            if boxed_answer:
                response, verify_cost = self.verify(problem, boxed_answer)
                cost = deep_cost + verify_cost
            else:
                cost = deep_cost
        else:  # difficult
            response, cost = self.deep_think(problem)

        return response, cost + diff_cost
    # EVOLVE-BLOCK-END


def run_experiment(**kwargs):
    """
    Entry point called by evaluate.py.

    Args:
        model_name: Name of the LLM model to use
        year: AIME dataset year
        max_calls: Maximum LLM calls per problem

    Returns:
        Tuple of (accuracy, cost, processed, num_llm_calls, dataframe)
    """
    from utils import query_llm, create_call_limited_query_llm
    from functools import partial
    from config import ROUTER_CONFIG

    base_query_llm = partial(query_llm, model_name=kwargs["model_name"])
    limited_query_llm = create_call_limited_query_llm(
        base_query_llm,
        max_calls=kwargs["max_calls"],
    )

    from math_eval import agent_evaluation

    accuracy, cost_total, processed, num_llm_calls, df = agent_evaluation(
        Agent,
        limited_query_llm,
        year=kwargs["year"],
        quick_temp=ROUTER_CONFIG.quick_temp,
        deep_temp=ROUTER_CONFIG.deep_temp,
        verify_temp=ROUTER_CONFIG.verify_temp,
        ensemble_size=ROUTER_CONFIG.ensemble_size,
    )

    return accuracy, cost_total, processed, num_llm_calls, df