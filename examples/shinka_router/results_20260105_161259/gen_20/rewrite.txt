"""ShinkaRouter: Agent with routing primitives for AIME problems.

This agent provides a toolbox of specialized primitives that can be
composed in different ways. The forward() method contains the routing
logic that will be evolved by ShinkaEvolve.
"""

import re
from typing import Callable, Tuple, List, Optional, Dict
from collections import Counter


class Agent:
    """Agent with multiple reasoning primitives for AIME math problems."""

    def __init__(
        self,
        query_llm: Callable,
        quick_temp: float = 0.7,
        deep_temp: float = 0.0,
        verify_temp: float = 0.0,
        ensemble_size: int = 3,
    ):
        """
        Initialize the routing agent.

        Args:
            query_llm: Function to query the LLM
            quick_temp: Temperature for quick_solve
            deep_temp: Temperature for deep_think
            verify_temp: Temperature for verify
            ensemble_size: Number of samples for ensemble voting
        """
        self.query_llm = query_llm
        self.quick_temp = quick_temp
        self.deep_temp = deep_temp
        self.verify_temp = verify_temp
        self.ensemble_size = ensemble_size

        # Standard output format for AIME
        self.output_format = (
            "On the final line output only the digits of the answer (0-999). "
            "Provide your final answer enclosed in a LaTeX \\boxed{{...}} command."
        )

        # Primitive call tracking (reset per forward() call)
        self._primitive_calls: List[str] = []

    def _track_call(self, primitive_name: str) -> None:
        """Track a primitive call for analysis."""
        self._primitive_calls.append(primitive_name)

    def get_primitive_calls(self) -> List[str]:
        """Return list of primitives called in this forward pass."""
        return self._primitive_calls.copy()

    def reset_tracking(self) -> None:
        """Reset primitive call tracking."""
        self._primitive_calls = []

    @staticmethod
    def extract_boxed_answer(response: str) -> Optional[str]:
        """Extract answer from \\boxed{} in response."""
        idx = response.rfind("\\boxed")
        if idx < 0:
            idx = response.rfind("\\fbox")
        if idx < 0:
            return None

        brace_idx = response.find("{", idx)
        if brace_idx < 0:
            return None

        level = 0
        for i in range(brace_idx, len(response)):
            if response[i] == "{":
                level += 1
            elif response[i] == "}":
                level -= 1
                if level == 0:
                    content = response[brace_idx + 1:i]
                    # Clean and normalize
                    content = content.strip().lstrip("0") or "0"
                    return content
        return None

    # ========================================================================
    # PRIMITIVE METHODS (Stable - Not Evolved)
    # ========================================================================

    def baseline_solve(self, problem: str) -> Tuple[str, float]:
        """Exact replication of adas_aime baseline behavior."""
        self._track_call("baseline_solve")

        system_prompt = "You are a skilled mathematician."
        task_prompt = f"{self.output_format}:\n\n{problem}\n\n"

        response, cost = self.query_llm(
            prompt=task_prompt,
            system=system_prompt,
            temperature=0.0,  # Deterministic
        )
        return response, cost

    def quick_solve(self, problem: str) -> Tuple[str, float]:
        """Fast solving good for easy problems or first-pass attempts."""
        self._track_call("quick_solve")

        system_prompt = "You are a skilled mathematician. Solve quickly and efficiently."
        task_prompt = f"{self.output_format}\n\n{problem}\n\n"

        response, cost = self.query_llm(
            prompt=task_prompt,
            system=system_prompt,
            temperature=self.quick_temp,
        )
        return response, cost

    def deep_think(self, problem: str) -> Tuple[str, float]:
        """Careful reasoning with chain-of-thought and low temperature."""
        self._track_call("deep_think")

        system_prompt = (
            "You are an expert mathematician. Think step-by-step, "
            "showing all your reasoning before arriving at the final answer."
        )
        task_prompt = (
            f"Solve this problem carefully with detailed reasoning:\n\n"
            f"{problem}\n\n"
            f"{self.output_format}"
        )

        response, cost = self.query_llm(
            prompt=task_prompt,
            system=system_prompt,
            temperature=self.deep_temp,
        )
        return response, cost

    def verify(self, problem: str, candidate_answer: str) -> Tuple[str, float]:
        """Act as a skeptical verifier checking a proposed solution."""
        self._track_call("verify")

        system_prompt = (
            "You are a rigorous mathematics professor checking a student's answer. "
            "Verify if the answer is correct. If wrong, provide the correct answer."
        )
        task_prompt = (
            f"Problem: {problem}\n\n"
            f"Proposed Answer: {candidate_answer}\n\n"
            f"Is this answer correct? If not, what is the correct answer?\n"
            f"{self.output_format}"
        )

        response, cost = self.query_llm(
            prompt=task_prompt,
            system=system_prompt,
            temperature=self.verify_temp,
        )
        return response, cost

    def ensemble_vote(self, problem: str, n: Optional[int] = None) -> Tuple[str, float]:
        """Generate multiple solutions and take majority vote on the answer."""
        self._track_call("ensemble_vote")

        if n is None:
            n = self.ensemble_size

        system_prompt = "You are a skilled mathematician."
        task_prompt = f"{self.output_format}\n\n{problem}\n\n"

        responses = []
        answers = []
        total_cost = 0.0

        for _ in range(n):
            response, cost = self.query_llm(
                prompt=task_prompt,
                system=system_prompt,
                temperature=0.7,  # Medium temp for diversity
            )
            responses.append(response)
            total_cost += cost

            ans = self.extract_boxed_answer(response)
            if ans:
                answers.append(ans)

        if answers:
            vote_counts = Counter(answers)
            winner, count = vote_counts.most_common(1)[0]

            for resp in responses:
                extracted = self.extract_boxed_answer(resp)
                if extracted == winner:
                    return resp, total_cost

        return responses[0], total_cost

    # ========================================================================
    # ROUTING LOGIC (Evolved by ShinkaEvolve)
    # ========================================================================

    # EVOLVE-BLOCK-START
    def forward(self, problem: str) -> Tuple[str, float]:
        """Main entry point that routes problem to appropriate primitives."""
        # Reset tracking for this problem
        self.reset_tracking()

        # Estimate the difficulty of the problem
        difficulty, cost = self.estimate_difficulty(problem)
        total_cost = cost

        if difficulty == 'easy':
            response, cost = self.quick_solve(problem)
            total_cost += cost
        elif difficulty == 'medium':
            response, cost = self.ensemble_vote(problem, n=3)
            total_cost += cost
        else:  # hard
            response, cost = self.deep_think(problem)
            total_cost += cost

            # Verify the answer
            answer = self.extract_boxed_answer(response)
            if answer:
                verify_response, verify_cost = self.verify(problem, answer)
                total_cost += verify_cost
                response = verify_response
            
        return response, total_cost
    # EVOLVE-BLOCK-END


def run_experiment(**kwargs):
    """
    Entry point called by evaluate.py.

    Args:
        model_name: Name of the LLM model to use
        year: AIME dataset year
        max_calls: Maximum LLM calls per problem

    Returns:
        Tuple of (accuracy, cost, processed, num_llm_calls, dataframe)
    """
    from utils import query_llm, create_call_limited_query_llm
    from functools import partial
    from config import ROUTER_CONFIG

    # Create base query_llm function
    base_query_llm = partial(query_llm, model_name=kwargs["model_name"])

    # Wrap it with call limiting
    limited_query_llm = create_call_limited_query_llm(
        base_query_llm,
        max_calls=kwargs["max_calls"],
    )

    # Import evaluation function
    from math_eval import agent_evaluation

    # Run evaluation with configured parameters
    accuracy, cost_total, processed, num_llm_calls, df = agent_evaluation(
        Agent,
        limited_query_llm,
        year=kwargs["year"],
        quick_temp=ROUTER_CONFIG.quick_temp,
        deep_temp=ROUTER_CONFIG.deep_temp,
        verify_temp=ROUTER_CONFIG.verify_temp,
        ensemble_size=ROUTER_CONFIG.ensemble_size,
    )

    return accuracy, cost_total, processed, num_llm_calls, df